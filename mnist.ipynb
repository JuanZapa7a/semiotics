{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanZapa7a/semiotics/blob/main/mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsenkQwcgV6H"
      },
      "source": [
        "# Quantization Aware Training (QAT) using Larq for binarized quantization with the MNIST dataset\n",
        "\n",
        "[Larq](https://larq.dev/) is a library designed to build and train binarised neural networks (BNNs) using TensorFlow and Keras. We are interested in performing hardware-aware training (considering noise, quantization, etc.) for deep models using Larq. We can achieve this by taking advantage of Larq's specific functionalities for binarisation and compact model training.\n",
        "\n",
        "Here is an outline of what we will cover in this notebook:\n",
        " 1. Installation of Larq and necessary dependencies.\n",
        " 2. Data preparation (MNIST).\n",
        " 3. Creation of a based model.\n",
        " 4. Training and evaluation of the based model .\n",
        " 5. Creation of a binarized model with QAT.\n",
        " 6. Training and evaluation of the binarized model.\n",
        " 7. Compare Models.\n",
        "\n",
        "This NoteBook uses [Larq](https://larq.dev/) and the [Keras Sequential API](https://www.tensorflow.org/guide/keras).\n",
        "\n",
        "The API of Larq is built on top of `tf.keras` and is designed to provide an easy to use, composable way to design and train BNNs (1 bit) and other types of Quantized Neural Networks (QNNs).\n",
        "\n",
        "It provides tools specifically designed to aid in BNN development, such as specialized optimizers, training metrics, and profiling tools.\n",
        "\n",
        "Note that efficient inference using a trained BNN requires the use of an optimized inference engine; we provide these for several platforms in [Larq Compute Engine](https://docs.larq.dev/compute-engine).\n",
        "\n",
        "To create a **Quantized Neural Network (QNN)**, Larq introduces two main components: **[quantized layers](https://docs.larq.dev/larq/api/layers/)** and **[quantizers](https://docs.larq.dev/larq/api/quantizers/)**.\n",
        "\n",
        "1. **Quantizers**: A quantizer defines two critical aspects:\n",
        "   - **Transformation of full-precision input to quantized output**: This involves converting high-precision values (usually 32-bit floating-point) to a lower-precision format (e.g., binary or integer). This reduces memory usage and computational load, which is helpful for efficiency.\n",
        "   - **Pseudo-gradient method for backpropagation**: Since quantization can create non-differentiable points, Larq uses an approximate or \"pseudo\" gradient method for the backward pass during training. This allows the model to still update weights even if the quantized values don't support traditional gradient computation.\n",
        "\n",
        "2. **Quantized Layers**: These layers use quantizers to handle activations and weights with reduced precision. Each quantized layer requires:\n",
        "   - **input_quantizer**: Defines how to quantize the incoming activations for the layer. This allows the model to operate on low-precision activations instead of full-precision ones.\n",
        "   - **kernel_quantizer**: Defines how to quantize the layer’s weights (often referred to as kernels in neural network layers).\n",
        "\n",
        "  If both `input_quantizer` and `kernel_quantizer` are set to `None`, then the layer behaves as a regular, full-precision layer, similar to standard TensorFlow/Keras layers.\n",
        "\n",
        "3. **Integration with Models**: These quantized layers can be added to a Keras model just like other layers. Alternatively, you can use them with a custom training loop if you need more control over the training process.\n",
        "\n",
        "Larq's QNN approach leverages quantizers to efficiently reduce precision while maintaining trainability through pseudo-gradients, which can then be integrated seamlessly into standard Keras workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installation of Larq and necessary dependencies"
      ],
      "metadata": {
        "id": "op030X-yW5Pv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAve6DCL4JH4"
      },
      "outputs": [],
      "source": [
        "!pip -q install tensorflow==2.10.0\n",
        "!pip -q install larq==0.13.1\n",
        "\n",
        "import tensorflow as tf\n",
        "import larq as lq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRFxccghyMVo"
      },
      "source": [
        "## 2. Data preparation (MNIST)\n",
        "\n",
        "Download and prepare the MNIST dataset.\n",
        "\n",
        "By default, each MNIST image has a shape of (28, 28), which is 2D.\n",
        "However, neural networks (especially convolutional networks) typically expect 3D inputs: (height, width, channels).\n",
        "Adding a channel dimension (with value 1 for grayscale) changes each image shape from (28, 28) to (28, 28, 1), which is required for most neural network layers to interpret the images correctly.\n",
        "The overall shapes for the dataset become (60000, 28, 28, 1) for training and (10000, 28, 28, 1) for testing.\n",
        "\n",
        "The MNIST dataset’s pixel values originally range from 0 to 255.\n",
        "Dividing by 127.5 and then subtracting 1 maps the values to a -1 to 1 range, which can help certain models converge more quickly and maintain numerical stability. (Centering pixel values around zero often benefits neural networks as it reduces bias and helps gradient-based methods perform better.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWoEqyMuXFF4"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)) # (60000, 28, 28) (60000,)\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)) # (10000, 28, 28) (10000,)\n",
        "\n",
        "# For binarized models, it is standard to normalize images to a range between -1 and 1.\n",
        "train_images, test_images = train_images / 127.5 - 1\n",
        "test_images / 127.5 - 1\n",
        "\n",
        "print(train_images.shape, train_labels.shape)  # Debe ser (60000, 28, 28, 1), (60000,)\n",
        "print(test_images.shape, test_labels.shape)    # Debe ser (10000, 28, 28, 1), (10000,)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Creation of a base model.\n",
        "\n",
        "To train the same model without binarization and quantization (i.e., using full precision for both weights and activations), we simply need to remove the QuantConv2D and QuantDense layers, replacing them with standard convolutional (Conv2D) and dense (Dense) layers from TensorFlow's Keras API."
      ],
      "metadata": {
        "id": "HL1KjCFYm1vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the standard (non-binarized) model architecture\n",
        "model_normal = tf.keras.models.Sequential()\n",
        "\n",
        "# First layer: Standard convolutional layer (no quantization)\n",
        "model_normal.add(tf.keras.layers.Conv2D(\n",
        "    32, (3, 3),                        # 32 filters of size 3x3\n",
        "    activation=\"relu\",                 # Use ReLU activation instead of binary\n",
        "    use_bias=False,                    # No bias for simplicity\n",
        "    input_shape=(28, 28, 1)            # Input shape for MNIST\n",
        "))\n",
        "model_normal.add(tf.keras.layers.MaxPooling2D((2, 2)))    # Max pooling for downsampling\n",
        "model_normal.add(tf.keras.layers.BatchNormalization(scale=False)) # Batch normalization\n",
        "\n",
        "# Second standard convolutional layer\n",
        "model_normal.add(tf.keras.layers.Conv2D(\n",
        "    64, (3, 3),\n",
        "    activation=\"relu\",\n",
        "    use_bias=False,\n",
        "))\n",
        "model_normal.add(tf.keras.layers.MaxPooling2D((2, 2)))     # Max pooling\n",
        "model_normal.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "\n",
        "# Third standard convolutional layer\n",
        "model_normal.add(tf.keras.layers.Conv2D(\n",
        "    64, (3, 3),\n",
        "    activation=\"relu\",\n",
        "    use_bias=False,\n",
        "))\n",
        "model_normal.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "model_normal.add(tf.keras.layers.Flatten())                # Flatten the output for the dense layers\n",
        "\n",
        "# Fully connected (dense) layer with 64 units\n",
        "model_normal.add(tf.keras.layers.Dense(64, activation=\"relu\", use_bias=False))\n",
        "model_normal.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "\n",
        "# Output layer for classification (10 classes for MNIST)\n",
        "model_normal.add(tf.keras.layers.Dense(10, activation=\"softmax\", use_bias=False))\n",
        "model_normal.add(tf.keras.layers.BatchNormalization(scale=False))\n"
      ],
      "metadata": {
        "id": "iAH3FF2rnAXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_normal.summary()"
      ],
      "metadata": {
        "id": "fXu_iWiinHrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training and evaluation of the normal model"
      ],
      "metadata": {
        "id": "lrL4fzR1nXfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model for normal training\n",
        "model_normal.compile(\n",
        "    optimizer='adam',                       # Adam optimizer\n",
        "    loss=\"sparse_categorical_crossentropy\", # Cross-entropy loss for classification\n",
        "    metrics=[\"accuracy\"]                    # Accuracy as the evaluation metric\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history_normal = model_normal.fit(\n",
        "    train_images, train_labels,\n",
        "    epochs=6,                        # Number of epochs\n",
        "    batch_size=64,                    # Batch size\n",
        "    validation_data=(test_images, test_labels)  # Use test data for validation\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss_normal, test_accuracy_normal = model_normal.evaluate(test_images, test_labels)\n",
        "\n",
        "print(f\"Test Accuracy (Normal): {test_accuracy_normal * 100:.2f}%\")\n",
        "print(f\"Test Loss(Normal): {test_loss_normal:.4f}\")"
      ],
      "metadata": {
        "id": "3tns10uNnezJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_normal.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_normal.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_normal.history['loss'], label='Train Loss')\n",
        "plt.plot(history_normal.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UHQ0T45Ji2gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oewp-wYg31t9"
      },
      "source": [
        "## 5. Creation of a binarized model\n",
        "\n",
        "The following will create a simple binarized CNN.\n",
        "\n",
        "The quantization function\n",
        "$$\n",
        "q(x) = \\begin{cases}\n",
        "    -1 & x < 0 \\\\\\\n",
        "    1 & x \\geq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "is used in the forward pass to binarize the activations and the latent full precision weights. The gradient of this function is zero almost everywhere which prevents the model from learning.\n",
        "\n",
        "To be able to train the model the gradient is instead estimated using the Straight-Through Estimator (STE)\n",
        "(the binarization is essentially replaced by a clipped identity on the backward pass):\n",
        "$$\n",
        "\\frac{\\partial q(x)}{\\partial x} = \\begin{cases}\n",
        "    1 & \\left|x\\right| \\leq 1 \\\\\\\n",
        "    0 & \\left|x\\right| > 1\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "In Larq this can be done by using `input_quantizer=\"ste_sign\"` and `kernel_quantizer=\"ste_sign\"`.\n",
        "Additionally, the latent full precision weights are clipped to -1 and 1 using `kernel_constraint=\"weight_clip\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9YmGQBQPrdn"
      },
      "outputs": [],
      "source": [
        "import larq as lq\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define default quantization options for all layers except the first layer\n",
        "kwargs = dict(\n",
        "    input_quantizer=\"ste_sign\",      # Quantizes activations using Sign-STE (binary quantization to -1 or +1)\n",
        "    kernel_quantizer=\"ste_sign\",     # Quantizes weights to binary values (-1 or +1) with Sign-STE\n",
        "    kernel_constraint=\"weight_clip\"  # Clips weights within a set range (typically -1 to +1) to stabilize training\n",
        ")\n",
        "\n",
        "# Initialize a Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# First layer: Quantized convolutional layer (only quantizing weights, not inputs)\n",
        "model.add(lq.layers.QuantConv2D(\n",
        "    32, (3, 3),                      # 32 filters of size 3x3\n",
        "    kernel_quantizer=\"ste_sign\",     # Quantize weights to -1 or +1\n",
        "    kernel_constraint=\"weight_clip\", # Restrict weights to a range for stability\n",
        "    use_bias=False,                  # Disable bias for simplicity\n",
        "    input_shape=(28, 28, 1)          # Input shape for MNIST (28x28 grayscale images)\n",
        "))\n",
        "model.add(tf.keras.layers.MaxPooling2D((2, 2)))         # Downsample with max pooling\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False)) # Batch normalization to stabilize activations\n",
        "\n",
        "# Second quantized convolutional layer (quantizes both weights and activations)\n",
        "model.add(lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs))\n",
        "model.add(tf.keras.layers.MaxPooling2D((2, 2)))         # Downsample again\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "\n",
        "# Third quantized convolutional layer (also quantizes both weights and activations)\n",
        "model.add(lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "model.add(tf.keras.layers.Flatten())                    # Flatten the output for fully connected layers\n",
        "\n",
        "# Quantized dense (fully connected) layer with 64 units\n",
        "model.add(lq.layers.QuantDense(64, use_bias=False, **kwargs))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "\n",
        "# Final quantized dense layer for output (10 classes for MNIST)\n",
        "model.add(lq.layers.QuantDense(10, use_bias=False, **kwargs))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "model.add(tf.keras.layers.Activation(\"softmax\"))        # Softmax activation for classification probabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvDVFkg-2DPm"
      },
      "source": [
        "Almost all parameters in the network are binarized, so either -1 or 1. This makes the network extremely fast if it would be deployed on custom BNN hardware.\n",
        "\n",
        " Here is the complete architecture of our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-C4XBg4UTJy"
      },
      "outputs": [],
      "source": [
        "lq.models.summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3odqfHP4M67"
      },
      "source": [
        "## 6. Training and evaluation of the binarized model with QAT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdDzI75PUXrG"
      },
      "outputs": [],
      "source": [
        "# Compile the model with an optimizer and loss function\n",
        "model.compile(\n",
        "    optimizer='adam',                      # Adam optimizer is often effective for training binarized networks\n",
        "    loss=\"sparse_categorical_crossentropy\",# Cross-entropy loss for classification\n",
        "    metrics=[\"accuracy\"]                   # Accuracy as the evaluation metric\n",
        ")\n",
        "\n",
        "# Train the model where QAT takes place as the model learns to optimize quantized weights and activations.\n",
        "history = model.fit(\n",
        "    train_images, train_labels,\n",
        "    epochs=6,                           # Number of training epochs\n",
        "    batch_size=64,                      # Batch size for training\n",
        "    validation_data=(test_images, test_labels) # Evaluate on test set after each epoch\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values for both models\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy (Binarized)')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy (Binarized)')\n",
        "plt.plot(history_normal.history['accuracy'], label='Train Accuracy (Normal)')\n",
        "plt.plot(history_normal.history['val_accuracy'], label='Validation Accuracy (Normal)')\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss (Binarized)')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss (Binarized)')\n",
        "plt.plot(history_normal.history['loss'], label='Train Loss (Normal)')\n",
        "plt.plot(history_normal.history['val_loss'], label='Validation Loss (Normal)')\n",
        "plt.title('Model Loss Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cXdkJ5d2oCp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the full training history\n",
        "\n",
        "1. **Training and Validation Accuracy**: Shows how well the model performs on the training and validation sets in terms of classification accuracy.\n",
        "2. **Training and Validation Loss**: Displays the loss values over epochs, providing insight into how well the model is learning and whether it might be overfitting."
      ],
      "metadata": {
        "id": "sYSGT02ZjIZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Compare models\n",
        "\n",
        "###**A. SIZE**\n",
        "\n",
        "We can evaluate how much memory each model takes up, specifically looking at the number of parameters (weights) in each model. The size of the model can be directly related to the total number of parameters, since each parameter is a floating-point value and requires memory for storage."
      ],
      "metadata": {
        "id": "7jE4e_Xhyv5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Get the Number of Parameters\n",
        "\n",
        "We can get the total number of parameters in both models by using the `summary()` method or by accessing the `count_params()` method for each layer."
      ],
      "metadata": {
        "id": "WtxvC1gpzAs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print summary of both models to compare parameters\n",
        "print(\"\\nBinarized Model Summary:\")\n",
        "model.summary()\n",
        "\n",
        "print(\"Normal Model Summary:\")\n",
        "model_normal.summary()"
      ],
      "metadata": {
        "id": "rF4SPUc8zOi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Step 2: Compare Model Size in Terms of Memory Usage\n",
        "\n",
        "Another way to compare model sizes is to look at the memory footprint, which depends on the number of parameters and their precision.\n",
        "\n",
        "#### Model Size Calculation in Bytes\n",
        "\n",
        "- **Normal Model**: Each parameter is a 32-bit floating-point value (4 bytes).\n",
        "- **Binarized Model**: Each parameter is a binary value, which can be represented in a single bit. However, when stored, these values are typically packed into bytes (so 1 byte per parameter).\n",
        "\n",
        "Let’s calculate the model size using the following formula:\n",
        "\n",
        "- **Normal Model Size (in bytes)**:\n",
        "  $$\n",
        "  \\text{Model Size} = \\text{Total Parameters} \\times 4 \\text{ bytes}\n",
        "  $$\n",
        "\n",
        "- **Binarized Model Size (in bytes)**:\n",
        "  $$\n",
        "  \\text{Model Size} = \\text{Total Parameters} \\times 1 \\text{ byte (because each parameter is binary)}\n",
        "  $$\n",
        "\n",
        "\n",
        "This will print the sizes of the two models in megabytes (MB), allowing you to easily compare the memory footprint.\n",
        "\n"
      ],
      "metadata": {
        "id": "b-Wgv020zaJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate the model size in bytes\n",
        "def calculate_model_size(model, is_binarized=False):\n",
        "    total_params = model.count_params()\n",
        "\n",
        "    if is_binarized:\n",
        "        # Binarized model parameters are stored in 1 byte per parameter\n",
        "        return total_params  # 1 byte per parameter for binarized model\n",
        "    else:\n",
        "        # Normal model parameters are stored in 4 bytes per parameter (32-bit floating point)\n",
        "        return total_params * 4  # 4 bytes per parameter for normal model\n",
        "\n",
        "# Get the sizes of both models\n",
        "normal_model_size = calculate_model_size(model_normal, is_binarized=False)\n",
        "binarized_model_size = calculate_model_size(model, is_binarized=True)\n",
        "\n",
        "# Print the model sizes in MB\n",
        "print(f\"Normal Model Size: {normal_model_size / (1024**2):.2f} MB\")\n",
        "print(f\"Binarized Model Size: {binarized_model_size / (1024**2):.2f} MB\")\n"
      ],
      "metadata": {
        "id": "uJMrez8fz-3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Save and Compare the File Sizes\n",
        "\n",
        "We can also save the models to disk in `.h5` or `.tf` format and compare their file sizes directly:\n",
        "\n",
        "This approach gives you the actual file sizes on disk, which could vary slightly due to additional overhead from the file format or model saving process, but it will still be a useful comparison.\n",
        "\n"
      ],
      "metadata": {
        "id": "5vyyQyb70NLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save both models to disk\n",
        "model.save(\"binarized_model.h5\")\n",
        "model_normal.save(\"normal_model.h5\")\n",
        "\n",
        "# Compare file sizes\n",
        "import os\n",
        "\n",
        "binarized_model_size = os.path.getsize(\"binarized_model.h5\") / (1024**2)\n",
        "normal_model_size = os.path.getsize(\"normal_model.h5\") / (1024**2)  # Size in MB\n",
        "\n",
        "print(f\"Binarized Model File Size: {binarized_model_size:.2f} MB\")\n",
        "print(f\"Normal Model File Size: {normal_model_size:.2f} MB\")"
      ],
      "metadata": {
        "id": "ZcMz7qzZ0TuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **B. Performance**\n",
        "\n",
        "To compare the performance of both models (the binarized model and the normal model), we can evaluate the following aspects:\n",
        "\n",
        "#### 1. **Inference Speed (Latency)**\n",
        "Binarized models tend to be faster in terms of inference because binary operations (like XOR) are more efficient than floating-point operations.\n",
        "   - **Latency**: This refers to the time it takes to make a prediction after the input data is passed through the model. The binarized model, which uses binary weights and activations, can be much faster than the normal model, since binary operations (XOR, bitwise operations) are generally faster than floating-point operations.\n",
        "   - **How to measure**: We can measure the time taken to make predictions on a batch of test data.\n",
        "\n",
        "#### 2. **Throughput (FPS - Frames Per Second)**\n",
        "The binarized model should also have higher throughput, meaning it can process more images per second.\n",
        "   - **Throughput**: This measures how many predictions the model can make per second. A higher throughput means better performance.\n",
        "   - **How to measure**: We can calculate how many images the model processes per second (images per second = batch size / time taken for prediction).\n",
        "\n",
        "#### 3. **Model Accuracy (Evaluation Metrics)**\n",
        "The normal model will typically have higher accuracy, but the binarized model’s performance can still be quite good for tasks like MNIST classification.\n",
        "   - **Accuracy**: Although not a direct \"performance\" measure in terms of speed, comparing the accuracy of the models on the same test set is essential. The normal model usually provides higher accuracy than the binarized model.\n",
        "   - **How to measure**: Use the test accuracy and loss from the evaluation of each model.\n",
        "\n",
        "#### 4. **Memory Usage during Inference**\n",
        "The binarized model will consume much less memory since the parameters are stored as binary values (1 bit per parameter), whereas the normal model uses 32-bit floating-point values (4 bytes per parameter).\n",
        "   - **Memory usage**: The normal model will consume more memory during inference due to larger parameter sizes. The binarized model requires less memory because it uses binary representations for weights and activations.\n",
        "   - **How to measure**: You can use system tools like `psutil` to monitor the memory usage while running inference.\n"
      ],
      "metadata": {
        "id": "83eu-Vbc1kll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Measure Inference Time (Latency)\n",
        "\n",
        "To measure inference time, use the `time` module to track how long it takes to perform inference on a test batch.\n",
        "\n",
        "This will give you a sense of how much faster the binarized model performs compared to the normal model."
      ],
      "metadata": {
        "id": "d8q9jLqv2TVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Function to measure inference time\n",
        "def measure_inference_time(model, test_images):\n",
        "    start_time = time.time()\n",
        "    predictions = model.predict(test_images)\n",
        "    end_time = time.time()\n",
        "\n",
        "    inference_time = end_time - start_time  # Time taken in seconds\n",
        "    return inference_time\n",
        "\n",
        "# Choose a batch of 100 test samples for inference time\n",
        "batch_size = 100\n",
        "test_batch = test_images[:batch_size]\n",
        "\n",
        "# Measure inference time for both models\n",
        "inference_time_normal = measure_inference_time(model_normal, test_batch)\n",
        "inference_time_binarized = measure_inference_time(model, test_batch)\n",
        "\n",
        "print(f\"Inference Time (Normal Model): {inference_time_normal:.4f} seconds\")\n",
        "print(f\"Inference Time (Binarized Model): {inference_time_binarized:.4f} seconds\")"
      ],
      "metadata": {
        "id": "al5-N2Nk2BuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Measure Throughput (FPS)\n",
        "\n",
        "Throughput can be measured by calculating how many images are processed per second. This will give us a sense of how much faster the binarized model performs compared to the normal model."
      ],
      "metadata": {
        "id": "1ZKHeQxB2zMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to measure throughput (images per second)\n",
        "def measure_throughput(model, test_images, batch_size):\n",
        "    start_time = time.time()\n",
        "    model.predict(test_images[:batch_size])  # Make prediction on a batch\n",
        "    end_time = time.time()\n",
        "\n",
        "    time_taken = end_time - start_time\n",
        "    throughput = batch_size / time_taken  # Images per second\n",
        "    return throughput\n",
        "\n",
        "# Measure throughput for both models\n",
        "throughput_normal = measure_throughput(model_normal, test_images, batch_size)\n",
        "throughput_binarized = measure_throughput(model, test_images, batch_size)\n",
        "\n",
        "print(f\"Throughput (Normal Model): {throughput_normal:.2f} images per second\")\n",
        "print(f\"Throughput (Binarized Model): {throughput_binarized:.2f} images per second\")"
      ],
      "metadata": {
        "id": "H6C4dWCF24bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 6: Compare Accuracy\n",
        "\n",
        "After training both models, evaluate their performance on the test data:\n"
      ],
      "metadata": {
        "id": "BWFc5SK23brq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the accuracy of both models\n",
        "test_loss_normal, test_accuracy_normal = model_normal.evaluate(test_images, test_labels)\n",
        "test_loss_binarized, test_accuracy_binarized = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print(f\"Test Accuracy (Normal Model): {test_accuracy_normal * 100:.2f}%\")\n",
        "print(f\"Test Accuracy (Binarized Model): {test_accuracy_binarized * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "lpH4IX403e0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Memory Usage (Optional)\n",
        "\n",
        "You can monitor the memory usage of each model during inference using a package like `psutil` (on most systems) or use platform-specific tools like `nvidia-smi` for GPU memory usage:"
      ],
      "metadata": {
        "id": "A-n634C13ww3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import os\n",
        "\n",
        "# Function to get current memory usage\n",
        "def get_memory_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
        "\n",
        "# Measure memory usage during inference for both models\n",
        "memory_usage_normal_start = get_memory_usage()\n",
        "model_normal.predict(test_batch)  # Inference on the normal model\n",
        "memory_usage_normal_end = get_memory_usage()\n",
        "\n",
        "memory_usage_binarized_start = get_memory_usage()\n",
        "model.predict(test_batch)  # Inference on the binarized model\n",
        "memory_usage_binarized_end = get_memory_usage()\n",
        "\n",
        "print(f\"Memory Usage (Normal Model): {memory_usage_normal_end - memory_usage_normal_start:.2f} MB\")\n",
        "print(f\"Memory Usage (Binarized Model): {memory_usage_binarized_end - memory_usage_binarized_start:.2f} MB\")"
      ],
      "metadata": {
        "id": "MomWvnsv30g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **C. Speed**\n",
        "\n",
        "In terms of **speed**, the primary difference between a **normal model** (using full precision floating-point values) and a **binarized model** (using binary values for weights and activations) comes down to the computational efficiency of operations. We can expect:\n",
        "\n",
        "1. **Reduced Computational Complexity**:\n",
        "   - Binarized models use binary values for weights and activations. Binary operations like **XOR** (exclusive or) and **bitwise operations** are computationally cheaper than floating-point operations like multiplication and addition.\n",
        "   - In a normal model, for each operation (like a convolution or matrix multiplication), floating-point multiplications and additions are performed, which take more time.\n",
        "   - In a binarized model, the binary values can be processed with efficient bitwise operations, which are typically **faster** than floating-point operations.\n",
        "\n",
        "2. **Optimized Memory Access**:\n",
        "   - Binary weights can be packed into smaller data structures (e.g., bit-packed arrays), which allows for faster memory access, fewer cache misses, and reduced bandwidth usage.\n",
        "   - Since binary weights are stored in 1 bit per weight, the data being fetched is much smaller compared to a normal model, which stores 32-bit floating-point numbers for each weight. This can reduce the time it takes to load the model's weights into memory and process them.\n",
        "\n",
        "3. **Fewer Operations**:\n",
        "   - Because binarized models have only two possible states (1 or -1 for weights), convolution and fully-connected operations involve simpler calculations. For example:\n",
        "     - **Dot product** in a normal model involves multiplications and additions for each weight. In a binarized model, the same operation can be reduced to **XOR** and simple summations.\n",
        "     - The complexity of these operations reduces significantly when you binarize the network, resulting in a faster forward pass during inference.\n",
        "\n",
        "4. **Potential for Hardware Acceleration**:\n",
        "   - Binarized models can be optimized further on specialized hardware, such as **ASICs** or **FPGAs**, which can accelerate binary operations much faster than standard CPUs or GPUs.\n",
        "   - Some hardware is designed to take advantage of binary operations, making them much faster than regular floating-point operations.\n",
        "\n"
      ],
      "metadata": {
        "id": "dOAEr_s74vZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Measure Speed (Inference Time)\n",
        "\n",
        "To compare the **speed** (in terms of inference time) between the normal and binarized models, you can follow this process:\n",
        "\n",
        "**Use Time Tracking**:\n",
        "   1. We can measure the inference time by tracking how long it takes for each model to make predictions on a batch of test data.\n",
        "   2. **Binarized models** should take less time compared to normal models, especially for larger models or datasets where the computational savings become more noticeable.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qba04vfM5bEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Function to measure inference time\n",
        "def measure_inference_time(model, test_images, batch_size=100):\n",
        "    start_time = time.time()\n",
        "    model.predict(test_images[:batch_size])  # Make prediction on a batch\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time  # Time taken in seconds\n",
        "\n",
        "# Measure inference time for both models\n",
        "batch_size = 100\n",
        "test_batch = test_images[:batch_size]  # Select a batch of 100 samples for comparison\n",
        "\n",
        "# Measure inference time for normal model\n",
        "inference_time_normal = measure_inference_time(model_normal, test_batch)\n",
        "# Measure inference time for binarized model\n",
        "inference_time_binarized = measure_inference_time(model, test_batch)\n",
        "\n",
        "print(f\"Inference Time for Normal Model: {inference_time_normal:.4f} seconds\")\n",
        "print(f\"Inference Time for Binarized Model: {inference_time_binarized:.4f} seconds\")"
      ],
      "metadata": {
        "id": "IzeqzoSV5hGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. **Conclusion: Comparison of Model Size, Performance, and Speed**\n",
        "\n",
        "When comparing a **binarized model** and a **normal model** in terms of **size**, **performance**, and **speed**, the results typically show clear differences that highlight the strengths and trade-offs of each approach. Here's a breakdown of the expected conclusions and values for each factor:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Model Size**\n",
        "- **Binarized Model**:\n",
        "  - **Smaller Size**: A binarized model uses **binary weights** (1 bit per weight), so the overall model size is drastically reduced. For example, if a normal model has 1 million parameters with 32-bit floating-point weights, the binarized model will only use **1/32 of the storage** for the weights, resulting in a model that is **32 times smaller**.\n",
        "  - **Expected Size**: For example, if the normal model is 50MB, the binarized model would be approximately **1.5MB**.\n",
        "  \n",
        "- **Normal Model**:\n",
        "  - **Larger Size**: The normal model uses **32-bit floating-point weights** for its parameters, which results in a larger model size.\n",
        "  - **Expected Size**: A typical normal model with 1 million parameters might be around **50MB** in size (using 32-bit precision).\n",
        "\n",
        "**Summary for Model Size**:\n",
        "- **Binarized Model**: Much smaller in size due to reduced precision (binary weights).\n",
        "- **Normal Model**: Larger due to full-precision (32-bit) weights.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Performance (Accuracy)**\n",
        "- **Binarized Model**:\n",
        "  - **Lower Accuracy**: While the binarized model is faster and smaller, it typically has lower **accuracy** compared to the normal model because reducing the precision of the weights and activations introduces approximation errors.\n",
        "  - **Expected Accuracy**: The binarized model may achieve **90-95% accuracy** on a dataset like MNIST, but it will generally be less precise than the normal model.\n",
        "\n",
        "- **Normal Model**:\n",
        "  - **Higher Accuracy**: The full-precision normal model typically delivers **higher accuracy**, as floating-point operations allow more precise calculations.\n",
        "  - **Expected Accuracy**: The normal model should achieve **98-99% accuracy** on datasets like MNIST.\n",
        "\n",
        "**Summary for Performance (Accuracy)**:\n",
        "- **Binarized Model**: Achieves slightly lower accuracy but remains competitive.\n",
        "- **Normal Model**: Achieves higher accuracy, but at the cost of increased model size and computational requirements.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Speed (Inference Time and Throughput)**\n",
        "\n",
        "- **Binarized Model**:\n",
        "  - **Faster Inference**: The binarized model should have **much faster inference** time because binary operations (like XOR and bitwise summation) are computationally cheaper than floating-point operations. This results in faster processing of input data.\n",
        "  - **Higher Throughput**: The binarized model can process more images per second (higher FPS) because it performs less complex computations, making it ideal for real-time applications or devices with limited resources.\n",
        "  - **Expected Inference Time**: A binarized model might process 100 images in **0.05 seconds**, compared to the normal model’s **0.12 seconds** for the same batch size.\n",
        "  \n",
        "- **Normal Model**:\n",
        "  - **Slower Inference**: The normal model takes more time per inference due to the heavier computational load of floating-point operations (multiplications and additions).\n",
        "  - **Lower Throughput**: Because it requires more time per image, the throughput (images per second) will be lower.\n",
        "  - **Expected Inference Time**: The normal model might take **0.12 seconds** for processing a batch of 100 images, compared to the binarized model’s **0.05 seconds**.\n",
        "  \n",
        "**Summary for Speed**:\n",
        "- **Binarized Model**: Significantly faster due to simpler binary operations, resulting in reduced inference time and higher throughput.\n",
        "- **Normal Model**: Slower due to more complex floating-point operations.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Memory Usage (During Inference)**\n",
        "- **Binarized Model**:\n",
        "  - **Lower Memory Usage**: The reduced size of the model (1 bit per weight) means that less memory is required to store the weights and activations. This is especially useful in **resource-constrained environments** such as mobile devices or edge computing.\n",
        "  - **Expected Memory Usage**: A binarized model might use **a fraction of the memory** compared to a normal model (e.g., 1MB vs. 50MB).\n",
        "\n",
        "- **Normal Model**:\n",
        "  - **Higher Memory Usage**: The normal model will require **more memory** because each weight is stored as a 32-bit floating-point number.\n",
        "  - **Expected Memory Usage**: A normal model might use **50MB** or more for storing the weights, depending on the number of parameters.\n",
        "\n",
        "**Summary for Memory Usage**:\n",
        "- **Binarized Model**: Much lower memory usage due to binary weights.\n",
        "- **Normal Model**: Higher memory usage due to 32-bit floating-point weights.\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Comparison Summary**\n",
        "\n",
        "| **Aspect**             | **Binarized Model**                  | **Normal Model**                      |\n",
        "|------------------------|--------------------------------------|---------------------------------------|\n",
        "| **Model Size**         | Much smaller (32x smaller)           | Larger (e.g., 50MB or more)           |\n",
        "| **Accuracy**           | Lower (90-95% for MNIST)             | Higher (98-99% for MNIST)             |\n",
        "| **Inference Speed**    | Much faster (e.g., 0.05s for 100 images) | Slower (e.g., 0.12s for 100 images)   |\n",
        "| **Throughput**         | Higher (more images per second)      | Lower (less images per second)        |\n",
        "| **Memory Usage**       | Lower (1MB vs 50MB)                  | Higher (50MB or more)                 |\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Conclusion**:\n",
        "\n",
        "- **Binarized models** offer significant **advantages in terms of size**, **speed**, and **memory usage**. They are ideal for deployment in **resource-constrained environments** (like mobile devices, embedded systems, or real-time applications). However, they come with the **trade-off of lower accuracy**, which is typically acceptable for many applications like image classification tasks (e.g., MNIST).\n",
        "  \n",
        "- **Normal models**, while offering **higher accuracy**, are **slower**, **larger**, and **require more memory**. They are suitable for environments where **accuracy is paramount** and computational resources are less constrained (e.g., cloud or high-performance systems).\n",
        "\n",
        "When choosing between these two approaches, the decision should be based on the trade-offs acceptable for the given application and deployment constraints."
      ],
      "metadata": {
        "id": "McA-AJyy7cLH"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "intro_to_cnns.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.14.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}