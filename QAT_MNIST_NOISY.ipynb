{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfE3HPIdcoqLVIqm03t+ux",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanZapa7a/semiotics/blob/main/QAT_MNIST_NOISY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installation of Larq and necessary dependencies"
      ],
      "metadata": {
        "id": "op030X-yW5Pv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAve6DCL4JH4"
      },
      "outputs": [],
      "source": [
        "!pip -q install tensorflow==2.10.0\n",
        "!pip -q install larq==0.13.1\n",
        "\n",
        "import tensorflow as tf\n",
        "import larq as lq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRFxccghyMVo"
      },
      "source": [
        "## 2. Data preparation (MNIST)\n",
        "\n",
        "Download and prepare the MNIST dataset.\n",
        "\n",
        "By default, each MNIST image has a shape of (28, 28), which is 2D.\n",
        "However, neural networks (especially convolutional networks) typically expect 3D inputs: (height, width, channels).\n",
        "Adding a channel dimension (with value 1 for grayscale) changes each image shape from (28, 28) to (28, 28, 1), which is required for most neural network layers to interpret the images correctly.\n",
        "The overall shapes for the dataset become (60000, 28, 28, 1) for training and (10000, 28, 28, 1) for testing.\n",
        "\n",
        "The MNIST dataset‚Äôs pixel values originally range from 0 to 255.\n",
        "Dividing by 127.5 and then subtracting 1 maps the values to a -1 to 1 range, which can help certain models converge more quickly and maintain numerical stability. (Centering pixel values around zero often benefits neural networks as it reduces bias and helps gradient-based methods perform better.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWoEqyMuXFF4"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)) # (60000, 28, 28) (60000,)\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)) # (10000, 28, 28) (10000,)\n",
        "\n",
        "# For binarized models, it is standard to normalize images to a range between -1 and 1.\n",
        "train_images, test_images = train_images / 127.5 - 1,test_images / 127.5 - 1\n",
        "\n",
        "print(train_images.shape, train_labels.shape)  # Debe ser (60000, 28, 28, 1), (60000,)\n",
        "print(test_images.shape, test_labels.shape)    # Debe ser (10000, 28, 28, 1), (10000,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oewp-wYg31t9"
      },
      "source": [
        "## 5. Creation of a binarized model\n",
        "\n",
        "The following will create a simple binarized CNN.\n",
        "\n",
        "The quantization function\n",
        "$$\n",
        "q(x) = \\begin{cases}\n",
        "    -1 & x < 0 \\\\\\\n",
        "    1 & x \\geq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "is used in the forward pass to binarize the activations and the latent full precision weights. The gradient of this function is zero almost everywhere which prevents the model from learning.\n",
        "\n",
        "To be able to train the model the gradient is instead estimated using the Straight-Through Estimator (STE)\n",
        "(the binarization is essentially replaced by a clipped identity on the backward pass):\n",
        "$$\n",
        "\\frac{\\partial q(x)}{\\partial x} = \\begin{cases}\n",
        "    1 & \\left|x\\right| \\leq 1 \\\\\\\n",
        "    0 & \\left|x\\right| > 1\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "In Larq this can be done by using `input_quantizer=\"ste_sign\"` and `kernel_quantizer=\"ste_sign\"`.\n",
        "Additionally, the latent full precision weights are clipped to -1 and 1 using `kernel_constraint=\"weight_clip\"`.\n",
        "\n",
        "```python\n",
        "import larq as lq\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define default quantization options for all layers except the first layer\n",
        "kwargs = dict(\n",
        "    input_quantizer=\"ste_sign\",      # Quantizes activations using Sign-STE (binary quantization to -1 or +1)\n",
        "    kernel_quantizer=\"ste_sign\",     # Quantizes weights to binary values (-1 or +1) with Sign-STE\n",
        "    kernel_constraint=\"weight_clip\"  # Clips weights within a set range (typically -1 to +1) to stabilize training\n",
        ")\n",
        "\n",
        "# Initialize a Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# First layer: Quantized convolutional layer (only quantizing weights, not inputs)\n",
        "model.add(lq.layers.QuantConv2D(\n",
        "    32, (3, 3),                      # 32 filters of size 3x3\n",
        "    kernel_quantizer=\"ste_sign\",     # Quantize weights to -1 or +1\n",
        "    kernel_constraint=\"weight_clip\", # Restrict weights to a range for stability\n",
        "    use_bias=False,                  # Disable bias for simplicity\n",
        "    input_shape=(28, 28, 1)          # Input shape for MNIST (28x28 grayscale images)\n",
        "))\n",
        "model.add(tf.keras.layers.MaxPooling2D((2, 2)))         # Downsample with max pooling\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False)) # Batch normalization to stabilize activations\n",
        "\n",
        "# Second quantized convolutional layer (quantizes both weights and activations)\n",
        "model.add(lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs))\n",
        "model.add(tf.keras.layers.MaxPooling2D((2, 2)))         # Downsample again\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "\n",
        "# Third quantized convolutional layer (also quantizes both weights and activations)\n",
        "model.add(lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "model.add(tf.keras.layers.Flatten())                    # Flatten the output for fully connected layers\n",
        "\n",
        "# Quantized dense (fully connected) layer with 64 units\n",
        "model.add(lq.layers.QuantDense(64, use_bias=False, **kwargs))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "\n",
        "# Final quantized dense layer for output (10 classes for MNIST)\n",
        "model.add(lq.layers.QuantDense(10, use_bias=False, **kwargs))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "model.add(tf.keras.layers.Activation(\"softmax\"))        # Softmax activation for classification probabilities\n",
        "\n",
        "lq.models.summary(model)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To add noise to the weights in your model's layers, you can create a custom layer that wraps the original layers and modifies the weights during training. Here's how you can modify your code to inject noise into the weights:\n",
        "\n",
        "### Modified code to add noise to the weights\n",
        "\n",
        "\n",
        "### Explanation of Changes:\n",
        "1. **`NoisyLayer` class**: This custom layer wraps an existing layer, such as `QuantConv2D` or `QuantDense`, and adds noise to its weights during training. The noise is Gaussian with a standard deviation defined by `noise_stddev`.\n",
        "2. **Adding noise**: Noise is added directly to the trainable weights using `assign_add`. This affects the weights during the forward pass.\n",
        "3. **Integration in the model**: All relevant layers are wrapped with instances of `NoisyLayer`.\n",
        "\n",
        "\n",
        "**Note**: The primary reason to use only the standard deviation (ùúé) when modeling noise in neural networks is that most common noise sources in hardware, like thermal or electronic noise, are typically assumed to follow a Gaussian distribution (normal distribution) with a mean (ùúá) of zero."
      ],
      "metadata": {
        "id": "4uD_yL9ozi5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import larq as lq\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define a custom layer to add noise to weights\n",
        "class NoisyLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, wrapped_layer, noise_mean=0.0, noise_stddev=0.05, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.wrapped_layer = wrapped_layer  # Original layer to wrap\n",
        "        self.noise_mean = noise_mean        # Mean of the noise\n",
        "        self.noise_stddev = noise_stddev    # Standard deviation of the noise\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply noise during both training and validation\n",
        "        for weight in self.wrapped_layer.trainable_weights:\n",
        "            noise = tf.random.normal(\n",
        "                shape=tf.shape(weight),\n",
        "                mean=self.noise_mean,\n",
        "                stddev=self.noise_stddev\n",
        "            )\n",
        "            noisy_weight = weight + noise  # Add noise to weights\n",
        "            # Apply the noisy weight to the layer\n",
        "            weight.assign(noisy_weight)  # Assign the noisy weights directly\n",
        "\n",
        "        return self.wrapped_layer(inputs)  # Apply the layer logic\n",
        "\n",
        "# Define default quantization options for quantized layers\n",
        "kwargs = dict(\n",
        "    input_quantizer=\"ste_sign\",\n",
        "    kernel_quantizer=\"ste_sign\",\n",
        "    kernel_constraint=\"weight_clip\"\n",
        ")\n",
        "\n",
        "# Initialize a Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# First convolutional layer with noise on weights\n",
        "model.add(NoisyLayer(\n",
        "    lq.layers.QuantConv2D(\n",
        "        32, (3, 3),\n",
        "        kernel_quantizer=\"ste_sign\",\n",
        "        kernel_constraint=\"weight_clip\",\n",
        "        use_bias=False,\n",
        "        input_shape=(28, 28, 1)\n",
        "    ),\n",
        "    noise_stddev=0.05  # Standard deviation of noise\n",
        "))\n",
        "\n",
        "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "\n",
        "# Second convolutional layer with noise on weights\n",
        "model.add(NoisyLayer(\n",
        "    lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs),\n",
        "    noise_stddev=0.05\n",
        "))\n",
        "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "\n",
        "# Third convolutional layer with noise on weights\n",
        "model.add(NoisyLayer(\n",
        "    lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs),\n",
        "    noise_stddev=0.05\n",
        "))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# Dense layer with noise on weights\n",
        "model.add(NoisyLayer(\n",
        "    lq.layers.QuantDense(64, use_bias=False, **kwargs),\n",
        "    noise_stddev=0.05\n",
        "))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "\n",
        "# Output layer with noise on weights\n",
        "model.add(NoisyLayer(\n",
        "    lq.layers.QuantDense(10, use_bias=False, **kwargs),\n",
        "    noise_stddev=0.05\n",
        "))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "model.add(tf.keras.layers.Activation(\"softmax\"))\n",
        "\n",
        "# Display the model summary\n",
        "lq.models.summary(model)"
      ],
      "metadata": {
        "id": "i09nnS7L2Zu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdDzI75PUXrG"
      },
      "outputs": [],
      "source": [
        "# Compile the model with an optimizer and loss function\n",
        "model.compile(\n",
        "    optimizer='adam',                      # Adam optimizer is often effective for training binarized networks\n",
        "    loss=\"sparse_categorical_crossentropy\",# Cross-entropy loss for classification\n",
        "    metrics=[\"accuracy\"]                   # Accuracy as the evaluation metric\n",
        ")\n",
        "\n",
        "# Train the model where QAT takes place as the model learns to optimize quantized weights and activations.\n",
        "history = model.fit(\n",
        "    train_images, train_labels,\n",
        "    epochs=6,                           # Number of training epochs\n",
        "    batch_size=64,                      # Batch size for training\n",
        "    validation_data=(test_images, test_labels) # Evaluate on test set after each epoch\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values for both models\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy (Binarized)')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy (Binarized)')\n",
        "plt.plot(history_normal.history['accuracy'], label='Train Accuracy (Normal)')\n",
        "plt.plot(history_normal.history['val_accuracy'], label='Validation Accuracy (Normal)')\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss (Binarized)')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss (Binarized)')\n",
        "plt.plot(history_normal.history['loss'], label='Train Loss (Normal)')\n",
        "plt.plot(history_normal.history['val_loss'], label='Validation Loss (Normal)')\n",
        "plt.title('Model Loss Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cXdkJ5d2oCp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the full training history\n",
        "\n",
        "1. **Training and Validation Accuracy**: Shows how well the model performs on the training and validation sets in terms of classification accuracy.\n",
        "2. **Training and Validation Loss**: Displays the loss values over epochs, providing insight into how well the model is learning and whether it might be overfitting."
      ],
      "metadata": {
        "id": "sYSGT02ZjIZN"
      }
    }
  ]
}