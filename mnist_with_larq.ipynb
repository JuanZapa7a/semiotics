{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanZapa7a/semiotics/blob/main/mnist_with_larq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsenkQwcgV6H"
      },
      "source": [
        "# Quantization Aware Training (QAT) using Larq for 4-bit quantization with the MNIST dataset\n",
        "\n",
        "[Larq](https://larq.dev/) is a library designed to build and train binarised neural networks (BNNs) using TensorFlow and Keras. If you are interested in performing hardware-aware training (considering noise, quantization, etc.) for deep models using Larq, you can achieve this by taking advantage of Larq's specific functionalities for binarisation and compact model training.\n",
        "\n",
        "Here is an outline of what we will cover:\n",
        " 1. Installation of Larq and necessary dependencies.\n",
        " 2. Data preparation (MNIST).\n",
        " 3. Creation of a base model (without quantization).\n",
        " 4. Training and evaluation of the base model.\n",
        " 5. Creation of a quantified model with QAT.\n",
        " 6. Training and evaluation of the quantized model.\n",
        " 7. Performance and model size comparison.\n",
        "\n",
        "Este NoteBook utiliza [Larq](https://larq.dev/) and the [Keras Sequential API](https://www.tensorflow.org/guide/keras).\n",
        "\n",
        "The API of Larq is built on top of `tf.keras` and is designed to provide an easy to use, composable way to design and train BNNs (1 bit) and other types of Quantized Neural Networks (QNNs).\n",
        "\n",
        "It provides tools specifically designed to aid in BNN development, such as specialized optimizers, training metrics, and profiling tools.\n",
        "\n",
        "Note that efficient inference using a trained BNN requires the use of an optimized inference engine; we provide these for several platforms in [Larq Compute Engine] (https://docs.larq.dev/compute-engine).\n",
        "\n",
        "To create a **Quantized Neural Network (QNN)**, Larq introduces two main components: **[quantized layers](https://docs.larq.dev/larq/api/layers/)** and **[quantizers](https://docs.larq.dev/larq/api/quantizers/)**.\n",
        "\n",
        "1. **Quantizers**: A quantizer defines two critical aspects:\n",
        "   - **Transformation of full-precision input to quantized output**: This involves converting high-precision values (usually 32-bit floating-point) to a lower-precision format (e.g., binary or integer). This reduces memory usage and computational load, which is helpful for efficiency.\n",
        "   - **Pseudo-gradient method for backpropagation**: Since quantization can create non-differentiable points, Larq uses an approximate or \"pseudo\" gradient method for the backward pass during training. This allows the model to still update weights even if the quantized values don't support traditional gradient computation.\n",
        "\n",
        "2. **Quantized Layers**: These layers use quantizers to handle activations and weights with reduced precision. Each quantized layer requires:\n",
        "   - **input_quantizer**: Defines how to quantize the incoming activations for the layer. This allows the model to operate on low-precision activations instead of full-precision ones.\n",
        "   - **kernel_quantizer**: Defines how to quantize the layerâ€™s weights (often referred to as kernels in neural network layers).\n",
        "\n",
        "If both `input_quantizer` and `kernel_quantizer` are set to `None`, then the layer behaves as a regular, full-precision layer, similar to standard TensorFlow/Keras layers.\n",
        "\n",
        "3. **Integration with Models**: These quantized layers can be added to a Keras model just like other layers. Alternatively, you can use them with a custom training loop if you need more control over the training process.\n",
        "\n",
        "Larq's QNN approach leverages quantizers to efficiently reduce precision while maintaining trainability through pseudo-gradients, which can then be integrated seamlessly into standard Keras workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installation of Larq and necessary dependencies."
      ],
      "metadata": {
        "id": "op030X-yW5Pv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAve6DCL4JH4"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.10.0\n",
        "!pip install larq==0.13.1\n",
        "\n",
        "import tensorflow as tf\n",
        "import larq as lq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRFxccghyMVo"
      },
      "source": [
        "### Download and prepare the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWoEqyMuXFF4"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "\n",
        "# Normalize pixel values to be between -1 and 1\n",
        "train_images, test_images = train_images / 127.5 - 1, test_images / 127.5 - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oewp-wYg31t9"
      },
      "source": [
        "### Create the model\n",
        "\n",
        "The following will create a simple binarized CNN.\n",
        "\n",
        "The quantization function\n",
        "$$\n",
        "q(x) = \\begin{cases}\n",
        "    -1 & x < 0 \\\\\\\n",
        "    1 & x \\geq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "is used in the forward pass to binarize the activations and the latent full precision weights. The gradient of this function is zero almost everywhere which prevents the model from learning.\n",
        "\n",
        "To be able to train the model the gradient is instead estimated using the Straight-Through Estimator (STE)\n",
        "(the binarization is essentially replaced by a clipped identity on the backward pass):\n",
        "$$\n",
        "\\frac{\\partial q(x)}{\\partial x} = \\begin{cases}\n",
        "    1 & \\left|x\\right| \\leq 1 \\\\\\\n",
        "    0 & \\left|x\\right| > 1\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "In Larq this can be done by using `input_quantizer=\"ste_sign\"` and `kernel_quantizer=\"ste_sign\"`.\n",
        "Additionally, the latent full precision weights are clipped to -1 and 1 using `kernel_constraint=\"weight_clip\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9YmGQBQPrdn"
      },
      "outputs": [],
      "source": [
        "# All quantized layers except the first will use the same options\n",
        "kwargs = dict(input_quantizer=\"ste_sign\",\n",
        "              kernel_quantizer=\"ste_sign\",\n",
        "              kernel_constraint=\"weight_clip\")\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# In the first layer we only quantize the weights and not the input\n",
        "model.add(lq.layers.QuantConv2D(32, (3, 3),\n",
        "                                kernel_quantizer=\"ste_sign\",\n",
        "                                kernel_constraint=\"weight_clip\",\n",
        "                                use_bias=False,\n",
        "                                input_shape=(28, 28, 1)))\n",
        "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "\n",
        "model.add(lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs))\n",
        "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "\n",
        "model.add(lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "model.add(lq.layers.QuantDense(64, use_bias=False, **kwargs))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "model.add(lq.layers.QuantDense(10, use_bias=False, **kwargs))\n",
        "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
        "model.add(tf.keras.layers.Activation(\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvDVFkg-2DPm"
      },
      "source": [
        "Almost all parameters in the network are binarized, so either -1 or 1. This makes the network extremely fast if it would be deployed on custom BNN hardware.\n",
        "\n",
        " Here is the complete architecture of our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-C4XBg4UTJy"
      },
      "outputs": [],
      "source": [
        "lq.models.summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3odqfHP4M67"
      },
      "source": [
        "### Compile and train the model\n",
        "\n",
        "Note: This may take a few minutes depending on your system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdDzI75PUXrG"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, batch_size=64, epochs=6)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKgyC5K_4O0d"
      },
      "source": [
        "### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LvwaKhtUdOo"
      },
      "outputs": [],
      "source": [
        "print(f\"Test accuracy {test_acc * 100:.2f} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cfJ8AR03gT5"
      },
      "source": [
        "As you can see, our simple binarized CNN has achieved a test accuracy of around 98 %. Not bad for a few lines of code!\n",
        "\n",
        "For information on converting Larq models to an optimized format and using or benchmarking them on Android or ARM devices, have a look at [this guide](https://docs.larq.dev/compute-engine/end_to_end/)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "intro_to_cnns.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.14.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}