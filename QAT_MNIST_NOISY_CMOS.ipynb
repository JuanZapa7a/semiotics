{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/JuanZapa7a/semiotics/blob/main/QAT_MNIST_NOISY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training (QAT) using Larq for a noisy CMOS binarized quantization with the CIFAR10 dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "op030X-yW5Pv"
   },
   "source": [
    "## 1. Installation of Larq and necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iAve6DCL4JH4"
   },
   "outputs": [],
   "source": [
    "!pip -q install tensorflow==2.10.0\n",
    "!pip -q install larq==0.13.1\n",
    "\n",
    "import tensorflow as tf\n",
    "import larq as lq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRFxccghyMVo"
   },
   "source": [
    "## 2. Data preparation (MNIST)\n",
    "\n",
    "Download and prepare the MNIST dataset.\n",
    "\n",
    "By default, each MNIST image has a shape of (28, 28), which is 2D.\n",
    "However, neural networks (especially convolutional networks) typically expect 3D inputs: (height, width, channels).\n",
    "Adding a channel dimension (with value 1 for grayscale) changes each image shape from (28, 28) to (28, 28, 1), which is required for most neural network layers to interpret the images correctly.\n",
    "The overall shapes for the dataset become (60000, 28, 28, 1) for training and (10000, 28, 28, 1) for testing.\n",
    "\n",
    "The MNIST dataset’s pixel values originally range from 0 to 255.\n",
    "Dividing by 127.5 and then subtracting 1 maps the values to a -1 to 1 range, which can help certain models converge more quickly and maintain numerical stability. (Centering pixel values around zero often benefits neural networks as it reduces bias and helps gradient-based methods perform better.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWoEqyMuXFF4",
    "outputId": "9292fbb9-261b-439f-d373-a40a1c390b88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n",
      "(60000, 28, 28, 1) (60000,)\n",
      "(10000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)) # (60000, 28, 28) (60000,)\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)) # (10000, 28, 28) (10000,)\n",
    "\n",
    "# For binarized models, it is standard to normalize images to a range between -1 and 1.\n",
    "train_images, test_images = train_images / 127.5 - 1,test_images / 127.5 - 1\n",
    "\n",
    "print(train_images.shape, train_labels.shape)  # Debe ser (60000, 28, 28, 1), (60000,)\n",
    "print(test_images.shape, test_labels.shape)    # Debe ser (10000, 28, 28, 1), (10000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AÑADIENDO RUIDO A LOS PESOS Y A LAS ACTIVACIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcfFb0N8fOrh"
   },
   "source": [
    "Para modificar el código y permitir que el ruido se inyecte tanto en los pesos como en las activaciones, podemos expandir la lógica de la clase `NoisyLayer`. Además, sí, las imágenes de entrada podrían venir con ruido (simulando ruido real del sensor o del ambiente), lo que podría mejorarse añadiendo ruido a las activaciones de cada capa.\n",
    "\n",
    "### **Código modificado para añadir ruido  a los pesos y activaciones:**\n",
    "\n",
    "1. **Inyectar ruido en las activaciones**:\n",
    "    - Modifica la función `call` de `NoisyLayer` para agregar ruido a las activaciones después de aplicar la lógica de la capa base.\n",
    "2. **Simular ruido en la entrada (opcional)**:\n",
    "    - Añade una capa adicional de ruido a las imágenes de entrada con `tf.keras.layers.GaussianNoise`.\n",
    "\n",
    "### **Explicación de los cambios**\n",
    "\n",
    "#### 1. **Ruido en activaciones**\n",
    "\n",
    "- Después de aplicar la lógica de la capa base (`self.wrapped_layer(inputs)`), se genera ruido para las activaciones con:\n",
    "    \n",
    "    ```python\n",
    "    activation_noise = tf.random.normal(\n",
    "        shape=tf.shape(outputs),\n",
    "        mean=self.noise_mean,\n",
    "        stddev=self.noise_stddev_activations\n",
    "    )\n",
    "    outputs_noisy = outputs + activation_noise\n",
    "    ```\n",
    "    \n",
    "- Esto inyecta ruido gaussiano en las salidas de cada capa.\n",
    "\n",
    "#### 2. **Simulación de imágenes de entrada con ruido**\n",
    "\n",
    "- La capa `tf.keras.layers.GaussianNoise` añade ruido gaussiano directamente a las imágenes de entrada:\n",
    "    \n",
    "    ```python\n",
    "    model.add(tf.keras.layers.GaussianNoise(0.05, input_shape=(28, 28, 1)))\n",
    "    ```\n",
    "    \n",
    "- Esto simula el ruido que podría provenir de sensores o entornos del mundo real.\n",
    "\n",
    "#### 3. **Parámetros separados para ruido en pesos y activaciones**\n",
    "\n",
    "- `noise_stddev_weights`: Desviación estándar para el ruido en los pesos.\n",
    "- `noise_stddev_activations`: Desviación estándar para el ruido en las activaciones.\n",
    "\n",
    "---\n",
    "\n",
    "### **Escenarios del ruido en imágenes**\n",
    "\n",
    "La simulación de ruido en las imágenes puede ser útil para:\n",
    "\n",
    "1. **Robustez a entradas ruidosas**:\n",
    "    - Por ejemplo, cámaras con baja iluminación, sensores defectuosos, o datos transmitidos con ruido.\n",
    "2. **Regularización adicional**:\n",
    "    - Similar a `Dropout`, añade variabilidad a las entradas, evitando que el modelo dependa de patrones específicos.\n",
    "\n",
    "---\n",
    "\n",
    "### **Resultados esperados**\n",
    "\n",
    "Con este enfoque:\n",
    "\n",
    "- El modelo será más robusto a perturbaciones tanto en las entradas como en las representaciones internas (activaciones y pesos).\n",
    "- Se puede usar para estudiar cómo el ruido afecta al rendimiento en diferentes partes del pipeline de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktrrzmFyfheO",
    "outputId": "52cf0c02-6ae7-4acb-c2ba-459b6e69df37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+sequential_1 stats---------------------------------------------------------------+\n",
      "| Layer                  Input prec.           Outputs  # 1-bit  # 32-bit  Memory |\n",
      "|                              (bit)                        x 1       x 1    (kB) |\n",
      "+---------------------------------------------------------------------------------+\n",
      "| gaussian_noise                   -   (-1, 28, 28, 1)        0         0       0 |\n",
      "| noisy_layer_5                    -  (-1, 26, 26, 32)      288         0    0.04 |\n",
      "| max_pooling2d_2                  -  (-1, 13, 13, 32)        0         0       0 |\n",
      "| batch_normalization_5            -  (-1, 13, 13, 32)        0        64    0.25 |\n",
      "| noisy_layer_6                    -  (-1, 11, 11, 64)    18432         0    2.25 |\n",
      "| max_pooling2d_3                  -    (-1, 5, 5, 64)        0         0       0 |\n",
      "| batch_normalization_6            -    (-1, 5, 5, 64)        0       128    0.50 |\n",
      "| noisy_layer_7                    -    (-1, 3, 3, 64)    36864         0    4.50 |\n",
      "| batch_normalization_7            -    (-1, 3, 3, 64)        0       128    0.50 |\n",
      "| flatten_1                        -         (-1, 576)        0         0       0 |\n",
      "| noisy_layer_8                    -          (-1, 64)    36864         0    4.50 |\n",
      "| batch_normalization_8            -          (-1, 64)        0       128    0.50 |\n",
      "| noisy_layer_9                    -          (-1, 10)      640         0    0.08 |\n",
      "| batch_normalization_9            -          (-1, 10)        0        20    0.08 |\n",
      "| activation_1                     -          (-1, 10)        0         0       0 |\n",
      "+---------------------------------------------------------------------------------+\n",
      "| Total                                                   93088       468   13.19 |\n",
      "+---------------------------------------------------------------------------------+\n",
      "+sequential_1 summary-----------------------+\n",
      "| Total params                   93.6 k     |\n",
      "| Trainable params               93.1 k     |\n",
      "| Non-trainable params           468        |\n",
      "| Model size                     13.19 KiB  |\n",
      "| Model size (8-bit FP weights)  11.82 KiB  |\n",
      "| Float-32 Equivalent            365.45 KiB |\n",
      "| Compression Ratio of Memory    0.04       |\n",
      "| Number of MACs                 0          |\n",
      "+-------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import larq as lq\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a custom layer to add noise to weights and activations\n",
    "class NoisyLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, wrapped_layer, noise_mean=0.0, noise_stddev_weights=0.01, noise_stddev_activations=0.01, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.wrapped_layer = wrapped_layer                 # Original layer to wrap\n",
    "        self.noise_mean = noise_mean                       # Mean of the noise\n",
    "        self.noise_stddev_weights = noise_stddev_weights   # Stddev of noise for weights\n",
    "        self.noise_stddev_activations = noise_stddev_activations  # Stddev of noise for activations\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Build the wrapped layer\n",
    "        self.wrapped_layer.build(input_shape)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Add noise to weights\n",
    "        for weight in self.wrapped_layer.trainable_weights:\n",
    "            noise = tf.random.normal(\n",
    "                shape=tf.shape(weight),\n",
    "                mean=self.noise_mean,\n",
    "                stddev=self.noise_stddev_weights\n",
    "            )\n",
    "            noisy_weight = weight + noise  # Add noise to weights\n",
    "            weight.assign(noisy_weight)   # Assign the noisy weights temporarily\n",
    "\n",
    "        # Forward pass through the wrapped layer\n",
    "        outputs = self.wrapped_layer(inputs)\n",
    "\n",
    "        # Add noise to activations\n",
    "        activation_noise = tf.random.normal(\n",
    "            shape=tf.shape(outputs),\n",
    "            mean=self.noise_mean,\n",
    "            stddev=self.noise_stddev_activations\n",
    "        )\n",
    "        outputs_noisy = outputs + activation_noise\n",
    "\n",
    "        return outputs_noisy\n",
    "\n",
    "# Define default quantization options for quantized layers\n",
    "kwargs = dict(\n",
    "    input_quantizer=\"ste_sign\",\n",
    "    kernel_quantizer=\"ste_sign\",\n",
    "    kernel_constraint=\"weight_clip\"\n",
    ")\n",
    "\n",
    "# Initialize a Sequential model\n",
    "model_wa = tf.keras.models.Sequential()\n",
    "\n",
    "# Add noise to input images (simulate noisy input images)\n",
    "model_wa.add(tf.keras.layers.GaussianNoise(0.05, input_shape=(28, 28, 1)))\n",
    "\n",
    "# First convolutional layer with noise on weights and activations\n",
    "model_wa.add(NoisyLayer(\n",
    "    lq.layers.QuantConv2D(\n",
    "        32, (3, 3),\n",
    "        kernel_quantizer=\"ste_sign\",\n",
    "        kernel_constraint=\"weight_clip\",\n",
    "        use_bias=False\n",
    "    ),\n",
    "    noise_stddev_weights=0.01,\n",
    "    noise_stddev_activations=0.01\n",
    "))\n",
    "model_wa.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_wa.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "# Second convolutional layer with noise on weights and activations\n",
    "model_wa.add(NoisyLayer(\n",
    "    lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs),\n",
    "    noise_stddev_weights=0.01,\n",
    "    noise_stddev_activations=0.01\n",
    "))\n",
    "model_wa.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_wa.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "# Third convolutional layer with noise on weights and activations\n",
    "model_wa.add(NoisyLayer(\n",
    "    lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs),\n",
    "    noise_stddev_weights=0.01,\n",
    "    noise_stddev_activations=0.01\n",
    "))\n",
    "model_wa.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model_wa.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Dense layer with noise on weights and activations\n",
    "model_wa.add(NoisyLayer(\n",
    "    lq.layers.QuantDense(64, use_bias=False, **kwargs),\n",
    "    noise_stddev_weights=0.01,\n",
    "    noise_stddev_activations=0.01\n",
    "))\n",
    "model_wa.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "# Output layer with noise on weights and activations\n",
    "model_wa.add(NoisyLayer(\n",
    "    lq.layers.QuantDense(10, use_bias=False, **kwargs),\n",
    "    noise_stddev_weights=0.01,\n",
    "    noise_stddev_activations=0.01\n",
    "))\n",
    "model_wa.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model_wa.add(tf.keras.layers.Activation(\"softmax\"))\n",
    "\n",
    "# Especificar la forma de entrada (28, 28, 1) para imágenes MNIST\n",
    "model_wa.build((None, 28, 28, 1))\n",
    "\n",
    "# Summarize the model with Larq\n",
    "lq.models.summary(model_wa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdDzI75PUXrG",
    "outputId": "910d1df8-f758-4887-efe2-49e4bf3e09f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "938/938 [==============================] - 141s 148ms/step - loss: 1.1043 - accuracy: 0.7392 - val_loss: 1.2303 - val_accuracy: 0.7123\n",
      "Epoch 2/6\n",
      "938/938 [==============================] - 138s 147ms/step - loss: 0.8255 - accuracy: 0.8575 - val_loss: 0.9434 - val_accuracy: 0.8340\n",
      "Epoch 3/6\n",
      "938/938 [==============================] - 129s 137ms/step - loss: 0.7172 - accuracy: 0.8957 - val_loss: 0.8199 - val_accuracy: 0.8880\n",
      "Epoch 4/6\n",
      "938/938 [==============================] - 139s 148ms/step - loss: 0.6532 - accuracy: 0.9152 - val_loss: 0.7405 - val_accuracy: 0.9021\n",
      "Epoch 5/6\n",
      "938/938 [==============================] - 138s 147ms/step - loss: 0.6049 - accuracy: 0.9311 - val_loss: 0.6318 - val_accuracy: 0.9383\n",
      "Epoch 6/6\n",
      "938/938 [==============================] - 139s 148ms/step - loss: 0.5873 - accuracy: 0.9377 - val_loss: 0.6666 - val_accuracy: 0.9358\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 0.8198 - accuracy: 0.9120\n",
      "Test Accuracy: 91.20%\n",
      "Test Loss: 0.8198\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with an optimizer and loss function\n",
    "model_wa.compile(\n",
    "    optimizer='adam',                      # Adam optimizer is often effective for training binarized networks\n",
    "    loss=\"sparse_categorical_crossentropy\",# Cross-entropy loss for classification\n",
    "    metrics=[\"accuracy\"]                   # Accuracy as the evaluation metric\n",
    ")\n",
    "\n",
    "# Train the model where QAT takes place as the model learns to optimize quantized weights and activations.\n",
    "history_wa = model_wa.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=6,                           # Number of training epochs\n",
    "    batch_size=64,                      # Batch size for training\n",
    "    validation_data=(test_images, test_labels), # Evaluate on test set after each epoch\n",
    "    shuffle=False                        # Shuffle the training data for each epoch\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model_wa.evaluate(test_images, test_labels)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. VARIACIONES DE PROCESO CMOS\n",
    "\n",
    "Las variaciones de proceso pueden cambiar los parámetros de los transistores, afectando el comportamiento de las operaciones aritméticas. Esto puede modelarse como una desviación en los pesos del modelo.\n",
    "\n",
    "### **Análisis del Código Actual**\n",
    "\n",
    "hemos definido un modelo de red neuronal convolucional en **TensorFlow** con capas cuantizadas usando **Larq**. Además, hemos implementado una capa personalizada (`NoisyLayer`) que introduce ruido gaussiano en los pesos y activaciones de otras capas para simular imprecisiones o ruido en el hardware en un modelo CNN llamado `model_wa` (**noise in weights and activations**).\n",
    "\n",
    "#### **Características**\n",
    "1. **Capa personalizada (`NoisyLayer`)**:\n",
    "   - **Pesos**: Introduce ruido gaussiano en los pesos de la capa antes de la operación `call`.\n",
    "   - **Activaciones**: Añade ruido gaussiano a las salidas de la capa.\n",
    "\n",
    "2. **Modelo secuencial (`model_wa`)**:\n",
    "   - Se utiliza ruido en los datos de entrada/imagen (`GaussianNoise`).\n",
    "   - Cada capa convolucional y densa está envuelta (wrapped) con `NoisyLayer` para simular hardware ruidoso.\n",
    "   - Usamos Larq (`lq`) para implementar las capas cuantizadas para QAT.\n",
    "\n",
    "3. **Finalidad del diseño**: Simular entornos con hardware real. El modelo es lo suficiente robusto frente al ruido inducido. \n",
    "\n",
    "---\n",
    "\n",
    "### **Introducir Variaciones del Proceso CMOS**\n",
    "\n",
    "El comportamiento de un circuito CMOS puede ser afectado por **variaciones de proceso**, que incluyen:\n",
    "- **Threshold Voltage Variation (Vt)**: Cambios en el umbral de los transistores afectan operaciones lógicas y de cuantización.\n",
    "- **Capacitive Coupling Noise**: Introduce inexactitudes adicionales.\n",
    "- **Temperature-induced Drift**: Cambios estocásticos en la precisión debido a temperatura.\n",
    "\n",
    "Para modelar estas variaciones, podemos extender el comportamiento del `NoisyLayer` para incluir:\n",
    "1. **Imprecisiones en operaciones aritméticas específicas** (e.g., multiplicación, suma).\n",
    "2. **Sesgo adicional en el ruido de pesos y activaciones basado en drift térmico o variación de Vt**.\n",
    "\n",
    "\n",
    "### **Explicación de Modificaciones**\n",
    "1. **Variación de Voltaje de Umbral (Vt)**:\n",
    "   - Añadimos un término multiplicativo `1 + vt_variation` en los pesos para modelar el efecto de variaciones estocásticas del proceso CMOS.\n",
    "\n",
    "2. **Ruido por Acoplamiento Capacitivo**:\n",
    "   - Introducimos un ruido gaussiano adicional (`capacitive_noise_stddev`) en las activaciones para reflejar la interferencia de acoplamiento.\n",
    "\n",
    "3. **Nueva Clase (`CMOSNoisyLayer`)**:\n",
    "   - Esta capa personalizada es una extensión de `NoisyLayer` que incluye las variaciones mencionadas.\n",
    "\n",
    "---\n",
    "\n",
    "Este modelo pretende ser `más realista` para simulaciones en hardware CMOS, ya que incorpora características específicas del proceso de fabricación y las operaciones aritméticas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import larq as lq\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a custom layer to add CMOS process variation effects\n",
    "class CMOSNoisyLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        wrapped_layer,\n",
    "        noise_mean=0.0,\n",
    "        noise_stddev_weights=0.01,\n",
    "        noise_stddev_activations=0.01,\n",
    "        vt_variation_factor=0.001,\n",
    "        capacitive_noise_stddev=0.005,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.wrapped_layer = wrapped_layer                 # Original layer to wrap\n",
    "        self.noise_mean = noise_mean                       # Mean of the noise\n",
    "        self.noise_stddev_weights = noise_stddev_weights   # Stddev of noise for weights\n",
    "        self.noise_stddev_activations = noise_stddev_activations  # Stddev of noise for activations\n",
    "        self.vt_variation_factor = vt_variation_factor     # Threshold voltage variation factor\n",
    "        self.capacitive_noise_stddev = capacitive_noise_stddev  # Capacitive coupling noise\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.wrapped_layer.build(input_shape)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Add noise to weights (including CMOS process variations)\n",
    "        for weight in self.wrapped_layer.trainable_weights:\n",
    "            noise = tf.random.normal(\n",
    "                shape=tf.shape(weight),\n",
    "                mean=self.noise_mean,\n",
    "                stddev=self.noise_stddev_weights\n",
    "            )\n",
    "            vt_variation = tf.random.uniform(\n",
    "                shape=tf.shape(weight),\n",
    "                minval=-self.vt_variation_factor,\n",
    "                maxval=self.vt_variation_factor\n",
    "            )\n",
    "            noisy_weight = weight * (1 + vt_variation) + noise  # Combine variations\n",
    "            weight.assign(noisy_weight)  # Assign noisy weights temporarily\n",
    "\n",
    "        # Forward pass through the wrapped layer\n",
    "        outputs = self.wrapped_layer(inputs)\n",
    "\n",
    "        # Add noise to activations\n",
    "        activation_noise = tf.random.normal(\n",
    "            shape=tf.shape(outputs),\n",
    "            mean=self.noise_mean,\n",
    "            stddev=self.noise_stddev_activations\n",
    "        )\n",
    "        capacitive_noise = tf.random.normal(\n",
    "            shape=tf.shape(outputs),\n",
    "            mean=0.0,\n",
    "            stddev=self.capacitive_noise_stddev\n",
    "        )\n",
    "        outputs_noisy = outputs + activation_noise + capacitive_noise  # Add combined noise\n",
    "\n",
    "        return outputs_noisy\n",
    "\n",
    "\n",
    "# Define default quantization options for quantized layers\n",
    "kwargs = dict(\n",
    "    input_quantizer=\"ste_sign\",\n",
    "    kernel_quantizer=\"ste_sign\",\n",
    "    kernel_constraint=\"weight_clip\"\n",
    ")\n",
    "\n",
    "# Initialize a Sequential model\n",
    "model_wa_cmos = tf.keras.models.Sequential()\n",
    "\n",
    "# Add noise to input images (simulate noisy input images)\n",
    "model_wa_cmos.add(tf.keras.layers.GaussianNoise(0.05, input_shape=(28, 28, 1)))\n",
    "\n",
    "# First convolutional layer with CMOS process noise\n",
    "model_wa_cmos.add(CMOSNoisyLayer(\n",
    "    lq.layers.QuantConv2D(\n",
    "        32, (3, 3),\n",
    "        kernel_quantizer=\"ste_sign\",\n",
    "        kernel_constraint=\"weight_clip\",\n",
    "        use_bias=False\n",
    "    ),\n",
    "    noise_stddev_weights=0.01,\n",
    "    noise_stddev_activations=0.01,\n",
    "    vt_variation_factor=0.002,\n",
    "    capacitive_noise_stddev=0.005\n",
    "))\n",
    "model_wa_cmos.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_wa_cmos.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "# Second convolutional layer\n",
    "model_wa_cmos.add(CMOSNoisyLayer(\n",
    "    lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs),\n",
    "    noise_stddev_weights=0.01,\n",
    "    noise_stddev_activations=0.01,\n",
    "    vt_variation_factor=0.002,\n",
    "    capacitive_noise_stddev=0.005\n",
    "))\n",
    "model_wa_cmos.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_wa_cmos.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "# Third convolutional layer\n",
    "model_wa_cmos.add(CMOSNoisyLayer(\n",
    "    lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs),\n",
    "    noise_stddev_weights=0.01,\n",
    "    noise_stddev_activations=0.01,\n",
    "    vt_variation_factor=0.002,\n",
    "    capacitive_noise_stddev=0.005\n",
    "))\n",
    "model_wa_cmos.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model_wa_cmos.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Dense layer\n",
    "model_wa_cmos.add(CMOSNoisyLayer(\n",
    "    lq.layers.QuantDense(64, use_bias=False, **kwargs),\n",
    "    noise_stddev_weights=0.01,\n",
    "    noise_stddev_activations=0.01,\n",
    "    vt_variation_factor=0.002,\n",
    "    capacitive_noise_stddev=0.005\n",
    "))\n",
    "model_wa_cmos.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "# Output layer\n",
    "model_wa_cmos.add(CMOSNoisyLayer(\n",
    "    lq.layers.QuantDense(10, use_bias=False, **kwargs),\n",
    "    noise_stddev_weights=0.01,\n",
    "    noise_stddev_activations=0.01,\n",
    "    vt_variation_factor=0.002,\n",
    "    capacitive_noise_stddev=0.005\n",
    "))\n",
    "model_wa_cmos.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model_wa_cmos.add(tf.keras.layers.Activation(\"softmax\"))\n",
    "\n",
    "# Especificar la forma de entrada (28, 28, 1) para imágenes MNIST\n",
    "model_wa_cmos.build((None, 28, 28, 1))\n",
    "\n",
    "# Summarize the model with Larq\n",
    "lq.models.summary(model_wa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "938/938 [==============================] - 141s 148ms/step - loss: 1.1043 - accuracy: 0.7392 - val_loss: 1.2303 - val_accuracy: 0.7123\n",
      "Epoch 2/6\n",
      "938/938 [==============================] - 138s 147ms/step - loss: 0.8255 - accuracy: 0.8575 - val_loss: 0.9434 - val_accuracy: 0.8340\n",
      "Epoch 3/6\n",
      "938/938 [==============================] - 129s 137ms/step - loss: 0.7172 - accuracy: 0.8957 - val_loss: 0.8199 - val_accuracy: 0.8880\n",
      "Epoch 4/6\n",
      "938/938 [==============================] - 139s 148ms/step - loss: 0.6532 - accuracy: 0.9152 - val_loss: 0.7405 - val_accuracy: 0.9021\n",
      "Epoch 5/6\n",
      "938/938 [==============================] - 138s 147ms/step - loss: 0.6049 - accuracy: 0.9311 - val_loss: 0.6318 - val_accuracy: 0.9383\n",
      "Epoch 6/6\n",
      "938/938 [==============================] - 139s 148ms/step - loss: 0.5873 - accuracy: 0.9377 - val_loss: 0.6666 - val_accuracy: 0.9358\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 0.8198 - accuracy: 0.9120\n",
      "Test Accuracy: 91.20%\n",
      "Test Loss: 0.8198\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with an optimizer and loss function\n",
    "model_wa_cmos.compile(\n",
    "    optimizer='adam',                      # Adam optimizer is often effective for training binarized networks\n",
    "    loss=\"sparse_categorical_crossentropy\",# Cross-entropy loss for classification\n",
    "    metrics=[\"accuracy\"]                   # Accuracy as the evaluation metric\n",
    ")\n",
    "\n",
    "# Train the model where QAT takes place as the model learns to optimize quantized weights and activations.\n",
    "history_wa_cmos = model_wa_cmos.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=6,                           # Number of training epochs\n",
    "    batch_size=64,                      # Batch size for training\n",
    "    validation_data=(test_images, test_labels), # Evaluate on test set after each epoch\n",
    "    shuffle=False                        # Shuffle the training data for each epoch\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model_wa_cmos.evaluate(test_images, test_labels)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOVlYKsVRFMJHMtEt4S6LAd",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
